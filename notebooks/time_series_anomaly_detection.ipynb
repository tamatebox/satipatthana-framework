{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Samadhi Model: Time Series Anomaly Detection (New Framework)\n",
    "\n",
    "このノートブックは、リファクタリングされた **Samadhi Framework** を使用して、時系列センサーデータに対する **異常検知** を実行する方法を示します。\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "異常検知プロセスは、以下の2つのフェーズに分かれています。\n",
    "\n",
    "1.  **Phase 1: Autoencoder Pre-training**\n",
    "    * **Objective:** \\`AutoencoderObjective\\`\n",
    "    * **Goal:** 複雑な認知モジュール (Vitakka/Vicara) を使用せず、モデルに正常な時系列パターンを再構築することを学習させます。これにより、「潜在空間マップ」が構築されます。\n",
    "\n",
    "2.  **Probe Initialization (Interim Step)**\n",
    "    * **Goal:** 事前学習された正常データに対してK-Meansクラスタリングを使用し、潜在空間内の「正常の中心」（Probes）を特定します。\n",
    "\n",
    "3.  **Phase 2: Main Training (Purification & Contrastive Learning)**\n",
    "    * **Objective:** \\`AnomalyObjective\\` (New!)\n",
    "    * **Goal:**\n",
    "        * **Attract Normal Data:** 再構築誤差を最小化し、思考プロセスを安定化させます（プローブへの収束）。\n",
    "        * **Repel Anomaly Data:** **Margin Loss** (コントラスト学習) を使用して再構築誤差を最大化します。\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1.  **Data Loading & Preprocessing:** センサーデータをロードし、時系列シーケンスを作成します。\n",
    "2.  **Model Instantiation:** \\`create_lstm_samadhi\\` ファクトリを使用します。\n",
    "3.  **Phase 1 Training:** 純粋なAutoencoderとして学習させます。\n",
    "4.  **Probe Initialization:** 正常データの潜在ベクトルをクラスタリングします。\n",
    "5.  **Phase 2 Training:** \\`AnomalyObjective\\` を使用して異常識別のため学習させます。\n",
    "6.  **Evaluation:** 再構築誤差と収束の安定性を分析して異常を検出します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Samadhi Framework Imports ---\n",
    "from src.configs.main import SamadhiConfig\n",
    "from src.presets.sequence import create_lstm_samadhi\n",
    "from src.train.hf_trainer import SamadhiTrainer\n",
    "from src.train.objectives.autoencoder import AutoencoderObjective\n",
    "from src.train.objectives.anomaly import AnomalyObjective\n",
    "\n",
    "print(f\"Torch device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "モデルと学習プロセスのハイパーパラメータを定義します。\\`SamadhiConfig\\` オブジェクトがこれらの設定を階層的に管理します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 32\n",
    "SEQ_LEN = 30\n",
    "N_PROBES = 10\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Configuration Dictionary\n",
    "config_dict = {\n",
    "    \"dim\": LATENT_DIM,\n",
    "    \"seed\": 42,\n",
    "    # --- Component Configs ---\n",
    "    \"adapter\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_dim\": None,  # Will be set after data loading\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "    },\n",
    "    \"vitakka\": {\n",
    "        \"n_probes\": N_PROBES,\n",
    "        \"attention_mode\": \"soft\", # Soft attention for training gradient flow\n",
    "    },\n",
    "    \"vicara\": {\n",
    "        \"refine_steps\": 5,\n",
    "        \"gate_threshold\": 0.5,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"hidden_dim\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "    },\n",
    "    \"objective\": {\n",
    "        \"stability_coeff\": 0.1,\n",
    "        \"entropy_coeff\": 0.05,\n",
    "        \"balance_coeff\": 1.0,\n",
    "        \"anomaly_margin\": 2.0,  # Margin for anomaly loss\n",
    "        \"anomaly_weight\": 1.0,  # Weight for anomaly penalty\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "## Data Loading & Preparation\n",
    "\n",
    "\\`sensor.csv\\` データセットをロードし、前処理を行い、学習セットとテストセットに分割します。また、Hugging Face Trainerと連携するためのカスタム \\`TorchDataset\\` を定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"x\": self.X[idx]} # Key 'x' matches SamadhiEngine forward signature\n",
    "        if self.y is not None:\n",
    "            item[\"y\"] = self.y[idx] # Key 'y' passed to Objective\n",
    "        return item\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    xs = []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        xs.append(data[i : (i + seq_len)])\n",
    "    return np.array(xs)\n",
    "\n",
    "def load_and_preprocess_data(path, seq_len):\n",
    "    print(f\"Loading data from {path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        df = pd.read_csv(\"../\" + path)\n",
    "\n",
    "    # Label Encoding\n",
    "    if 'machine_status' in df.columns:\n",
    "        df['label'] = df['machine_status'].apply(lambda x: 0 if x == 'NORMAL' else 1)\n",
    "        labels = df['label'].values\n",
    "    else:\n",
    "        labels = np.zeros(len(df))\n",
    "\n",
    "    # Drop columns\n",
    "    cols_to_drop = ['Unnamed: 0', 'timestamp', 'machine_status', 'label', 'Class', 'sensor_15']\n",
    "    features_df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "    features_df = features_df.fillna(method='ffill').fillna(0)\n",
    "\n",
    "    # Select numeric\n",
    "    numeric_cols = features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    features_df = features_df[numeric_cols]\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_df)\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = create_sequences(scaled_features, seq_len)\n",
    "    sequence_labels = labels[seq_len - 1 :]\n",
    "\n",
    "    # Split (Time-based)\n",
    "    split_train = 110000\n",
    "    split_test = 150000\n",
    "\n",
    "    X_train = sequences[:split_train]\n",
    "    y_train = sequence_labels[:split_train]\n",
    "\n",
    "    X_test = sequences[split_train:split_test]\n",
    "    y_test = sequence_labels[split_train:split_test]\n",
    "\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, y_test, sequences.shape[-1]\n",
    "\n",
    "# Load Data\n",
    "DATA_PATH = \"data/sensor.csv\"\n",
    "X_train, y_train, X_test, y_test, input_dim = load_and_preprocess_data(DATA_PATH, SEQ_LEN)\n",
    "\n",
    "# Update Config\n",
    "config_dict[\"adapter\"][\"input_dim\"] = input_dim\n",
    "config_dict[\"decoder\"][\"output_dim\"] = input_dim\n",
    "config = SamadhiConfig.from_dict(config_dict)\n",
    "print(f\"Config Input Dim set to: {config.adapter.input_dim}\")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset_full = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# For Pre-training (Normal Data Only)\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "y_train_normal = y_train[y_train == 0]\n",
    "train_dataset_normal = TimeSeriesDataset(X_train_normal, y_train_normal)\n",
    "\n",
    "print(f\"Pre-train dataset size (Normal only): {len(train_dataset_normal)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": [
    "## Model Instantiation\n",
    "\n",
    "\\`create_lstm_samadhi\\` ファクトリを使用してモデルを作成します。これは、LSTM AdapterとDecoderで構成された \\`SamadhiEngine\\` を構築します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_create",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lstm_samadhi(config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1",
   "metadata": {},
   "source": [
    "## Phase 1: Autoencoder Pre-training\n",
    "\n",
    "\\`AutoencoderObjective\\` を使用してモデルを学習させます。これにより、VitakkaとVicaraは無効化され、AdapterとDecoderのみが入力の再構築のために学習されます。ここでは **正常な** データのみを使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_phase1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 1. Define Objective\n",
    "ae_objective = AutoencoderObjective(config)\n",
    "\n",
    "# 2. Training Args\n",
    "args_phase1 = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=500,\n",
    "    remove_unused_columns=False, # Essential for custom datasets\n",
    "    report_to=\"none\"\n",
    "    )\n",
    "\n",
    "# 3. Trainer\n",
    "trainer_phase1 = SamadhiTrainer(\n",
    "    model=model,\n",
    "    args=args_phase1,\n",
    "    train_dataset=train_dataset_normal,\n",
    "    objective=ae_objective\n",
    "    )\n",
    "\n",
    "# 4. Train\n",
    "trainer_phase1.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "probe_init",
   "metadata": {},
   "source": [
    "## Probe Initialization\n",
    "\n",
    "Adapterが意味のある潜在ベクトルを生成できるようになったので、この潜在空間内の正常な学習データをクラスタリングして、Probesを初期化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_probes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_probes(model, dataset, n_probes, device):\n",
    "    print(\"Initializing probes via KMeans...\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # 1. Collect Latents\n",
    "    latents = []\n",
    "    loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            # Only run adapter\n",
    "            z = model.adapter(x)\n",
    "            latents.append(z.cpu().numpy())\n",
    "\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "\n",
    "    # Subsample if too large\n",
    "    if len(latents) > 20000:\n",
    "        indices = np.random.choice(len(latents), 20000, replace=False)\n",
    "        latents_sub = latents[indices]\n",
    "    else:\n",
    "        latents_sub = latents\n",
    "\n",
    "    # 2. KMeans\n",
    "    kmeans = KMeans(n_clusters=n_probes, random_state=42, n_init='auto')\n",
    "    kmeans.fit(latents_sub)\n",
    "    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 3. Set Probes\n",
    "    model.vitakka.probes.data = centroids\n",
    "    print(\"Probes initialized successfully.\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "initialize_probes(model, train_dataset_normal, N_PROBES, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2",
   "metadata": {},
   "source": [
    "## Phase 2: Main Training (Anomaly Objective)\n",
    "\n",
    "ここで \\`AnomalyObjective\\` に切り替えます。これにより、フルモデル（Vitakka探索 + Vicara洗練）が学習されます。\n",
    "\n",
    "* **Normal Data:** プローブに収束するように学習されます（再構築誤差の最小化）。\n",
    "* **Anomaly Data:** 「拒否」されるように学習されます（再構築誤差 > マージンの最大化）。\n",
    "\n",
    "このフェーズでは、異常を含むフル学習セットを使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_phase2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Anomaly Objective\n",
    "anomaly_objective = AnomalyObjective(config)\n",
    "\n",
    "# 2. Training Args\n",
    "args_phase2 = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=5e-4, # Slightly lower LR for fine-tuning\n",
    "    logging_steps=500,\n",
    "    remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "# 3. Trainer\n",
    "trainer_phase2 = SamadhiTrainer(\n",
    "    model=model,\n",
    "    args=args_phase2,\n",
    "    train_dataset=train_dataset_full, # Use full dataset with anomalies\n",
    "    objective=anomaly_objective\n",
    "    )\n",
    "\n",
    "# 4. Train\n",
    "trainer_phase2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval",
   "metadata": {},
   "source": [
    "## Evaluation & Analysis\n",
    "\n",
    "テストセットでモデルを評価します。以下の点に着目します。\n",
    "1.  **Reconstruction Error:** 正常データでは低く、異常データでは高くなるはずです。\n",
    "2.  **ROC-AUC:** 全体的な検出性能です。\n",
    "3.  **Visualizations:** 誤差の分布と散布図です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    recon_errors = []\n",
    "    labels = []\n",
    "    confidences = []\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y = batch.get(\"y\")\n",
    "\n",
    "            # Forward pass with full cognitive loop\n",
    "            # SamadhiEngine.forward(x, run_vitakka=True, run_vicara=True)\n",
    "            # Returns: output, s_final, meta\n",
    "            x_recon, s_final, meta = model(x, run_vitakka=True, run_vicara=True)\n",
    "\n",
    "            # Calculate MSE per sample\n",
    "            loss = torch.mean((x_recon - x)**2, dim=[1, 2])\n",
    "\n",
    "            recon_errors.append(loss.cpu().numpy())\n",
    "            confidences.append(meta[\"confidence\"].cpu().numpy())\n",
    "            if y is not None:\n",
    "                labels.append(y.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        np.concatenate(recon_errors),\n",
    "        np.concatenate(labels) if labels else None,\n",
    "        np.concatenate(confidences)\n",
    "    )\n",
    "\n",
    "# Run Evaluation\n",
    "recon_errs, y_true, confs = evaluate_model(model, test_dataset, device)\n",
    "\n",
    "# --- Metrics ---\n",
    "roc_score = roc_auc_score(y_true, recon_errs)\n",
    "print(f\"ROC-AUC Score: {roc_score:.4f}\")\n",
    "\n",
    "# Find Best Threshold (F1)\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, recon_errs)\n",
    "f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Best Threshold (Max F1): {best_threshold:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "y_pred = (recon_errs > best_threshold).astype(int)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Normal\", \"Fraud\"]))\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 1. Error Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(recon_errs[y_true==0], fill=True, label='Normal')\n",
    "sns.kdeplot(recon_errs[y_true==1], fill=True, label='Fraud')\n",
    "plt.title('Reconstruction Error Density')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "# 2. Scatter Plot (Conf vs Error)\n",
    "plt.subplot(1, 2, 2)\n",
    "# Subsample for scatter plot clarity\n",
    "idx_norm = np.where(y_true==0)[0]\n",
    "if len(idx_norm) > 2000:\n",
    "    idx_norm = np.random.choice(idx_norm, 2000, replace=False)\n",
    "idx_fraud = np.where(y_true==1)[0]\n",
    "\n",
    "plt.scatter(confs[idx_norm], recon_errs[idx_norm], alpha=0.3, s=10, label='Normal')\n",
    "plt.scatter(confs[idx_fraud], recon_errs[idx_fraud], alpha=0.6, s=20, color='red', label='Fraud')\n",
    "plt.title('Confidence vs Reconstruction Error')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samadhi-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
