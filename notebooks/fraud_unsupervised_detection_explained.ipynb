{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí≥ Samadhi Model - Credit Card Fraud Detection Demo (Explained)\n",
    "\n",
    "„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅSamadhi„É¢„Éá„É´Ôºà`MlpSamadhiModel`Ôºâ„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ**„ÇØ„É¨„Ç∏„ÉÉ„Éà„Ç´„Éº„Éâ„ÅÆ‰∏çÊ≠£Âà©Áî®Ê§úÁü•ÔºàÁï∞Â∏∏Ê§úÁü•Ôºâ**„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° „Ç¢„Éó„É≠„Éº„ÉÅ: ÂçäÊïôÂ∏´„ÅÇ„ÇäÁï∞Â∏∏Ê§úÁü• (Semi-Supervised Anomaly Detection)\n",
    "\n",
    "Samadhi„É¢„Éá„É´„ÅØ„ÄÅÂçò„Å™„Çã„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÅØÁï∞„Å™„Çä„ÄÅÂÖ•Âäõ„Éá„Éº„Çø„ÇíËÉΩÂãïÁöÑ„Å´„ÄåÊ≠£Â∏∏„Å™Ê¶ÇÂøµÔºàProbeÔºâ„Äç„Å´Âºï„ÅçÂØÑ„Åõ„Å¶Á¥îÂåñ„Åó„Çà„ÅÜ„Å®„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÁï∞Â∏∏„Éá„Éº„Çø„Å´ÂØæ„Åô„ÇãÊÑüÂ∫¶„ÅåÈ´ò„Åæ„Çä„ÄÅËß£ÈáàÊÄß„ÇÇÊèê‰æõ„Åï„Çå„Åæ„Åô„ÄÇ\n",
    "\n",
    "1.  **Ê≠£Â∏∏„Éë„Çø„Éº„É≥„ÅÆÂ≠¶Áøí**: Ê≠£Â∏∏„Å™ÂèñÂºï„Éá„Éº„Çø„ÅÆ„Åø„ÇíÁî®„ÅÑ„Å¶„ÄÅSamadhi„É¢„Éá„É´„Å´„ÄåÊ≠£Â∏∏„Å®„ÅØ‰Ωï„Åã„Äç„ÇíÊ∑±„ÅèÂ≠¶Áøí„Åï„Åõ„Åæ„ÅôÔºà**Ê¶ÇÂøµÊé¢Á¥¢**„Å®**Á¥îÂåñ**Ôºâ„ÄÇ\n",
    "2.  **Áï∞Â∏∏„ÅÆÊ§úÂá∫**: Â≠¶Áøí„Åó„Åü„É¢„Éá„É´„Å´„Éá„Éº„Çø„ÇíÂÖ•Âäõ„Åó„ÄÅÊ≠£Â∏∏„Å™Ê¶ÇÂøµ„Å´ÂΩì„Å¶„ÅØ„Åæ„Çâ„Å™„ÅÑÔºà**ÂÜçÊßãÊàêË™§Â∑Æ„ÅåÂ§ß„Åç„ÅÑ**Ôºâ„ÇÇ„ÅÆ„Çí„Äå‰∏çÊ≠£„Äç„Å®Âà§ÂÆö„Åó„Åæ„Åô„ÄÇ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924c31c",
   "metadata": {},
   "source": [
    "\n",
    "# üßò Samadhi„É¢„Éá„É´„ÅÆ‰∏ªË¶Å„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÊßãÊàê\n",
    "\n",
    "„Åì„ÅÆ„Éá„É¢„Åß‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Çã Samadhi „É¢„Éá„É´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅßÊßãÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„ÅØ `CONFIG_DICT` „ÅÆË®≠ÂÆö„Å´Âü∫„Å•„Åç„ÄÅ`create_mlp_samadhi` Èñ¢Êï∞„Å´„Çà„Å£„Å¶Ëá™ÂãïÁöÑ„Å´ÁµÑ„ÅøÁ´ã„Å¶„Çâ„Çå„Åæ„Åô„ÄÇ\n",
    "\n",
    "-----\n",
    "\n",
    "## üìä Adapter (`MlpAdapter`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: ÂÖ•Âäõ„Éá„Éº„ÇøÔºà29Ê¨°ÂÖÉ„ÅÆË°®ÂΩ¢Âºè„Éá„Éº„ÇøÔºâ„Çí**ÊΩúÂú®Á©∫Èñì„ÅÆË°®Áèæ**ÔºàÊ¨°ÂÖÉÊï∞32Ôºâ„Å´Â§âÊèõ„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `mlp` (Â§öÂ±§„Éë„Éº„Çª„Éó„Éà„É≠„É≥ - Ë°®ÂΩ¢Âºè„Éá„Éº„ÇøÁî®)\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"adapter\": {\n",
    "    \"type\": \"mlp\",\n",
    "    \"input_dim\": 29,       // „ÇØ„É¨„Ç∏„ÉÉ„Éà„Ç´„Éº„ÉâÂèñÂºï„Éá„Éº„Çø„ÅÆÁâπÂæ¥ÈáèÊï∞\n",
    "    \"hidden_dim\": 64\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## üîÑ Decoder (`ReconstructionDecoder`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: ÊΩúÂú®Á©∫Èñì„ÅÆË°®ÁèæÔºàÊ¨°ÂÖÉÊï∞32Ôºâ„Åã„Çâ**ÂÖÉ„ÅÆÂÖ•Âäõ„Éá„Éº„Çø**Ôºà29Ê¨°ÂÖÉÔºâ„ÇíÂÜçÊßãÊàê„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `reconstruction` (ÂÖ•ÂäõÂæ©ÂÖÉÁî®)\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"decoder\": {\n",
    "    \"type\": \"reconstruction\",\n",
    "    \"input_dim\": 29,\n",
    "    \"decoder_hidden_dim\": 64\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## üîç Vitakka (`StandardVitakka`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: ÂÖ•Âäõ„Åï„Çå„ÅüÊΩúÂú®Áä∂ÊÖã„Åã„Çâ„ÄÅÊúÄ„ÇÇÈñ¢ÈÄ£ÊÄß„ÅÆÈ´ò„ÅÑ„Äå**Ê¶ÇÂøµÔºàProbeÔºâ**„Äç„ÇíËÉΩÂãïÁöÑ„Å´Êé¢Á¥¢„Åó„ÄÅÊÄùËÄÉ„ÅÆÂàùÊúüÁä∂ÊÖã $\\mathbf{s}_0$ „ÇíÁîüÊàê„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `standard`\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"vitakka\": {\n",
    "    \"n_probes\": 5,                     // Ê≠£Â∏∏„Å™ÂèñÂºï„Éë„Çø„Éº„É≥„Å´ÂØæÂøú„Åô„Çã5ÂÄã„ÅÆ„Éó„É≠„Éº„Éñ\n",
    "    \"gate_threshold\": 0.5,             // „Ç≤„Éº„Éà„ÇíÈñã„Åè„Åü„ÇÅ„ÅÆÈñæÂÄ§Ôºà‰ø°È†ºÂ∫¶„ÅåÈ´ò„Åë„Çå„Å∞ÈÄöÈÅéÔºâ\n",
    "    \"mix_alpha\": 0.5,                  // ÂÖ•Âäõ„Å®„Éó„É≠„Éº„Éñ„ÅÆÊ∑∑ÂêàÊØîÁéá\n",
    "    \"softmax_temp\": 0.5,               // Á¢∫ÁéáÂàÜÂ∏É„ÅÆ„Ç∑„É£„Éº„Éó„Éç„ÇπË™øÊï¥\n",
    "    \"training_attention_mode\": \"soft\", // Â≠¶ÁøíÊôÇ: ÂæÆÂàÜÂèØËÉΩ„Å™„ÇΩ„Éï„Éà„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "    \"prediction_attention_mode\": \"hard\"  // Êé®Ë´ñÊôÇ: ÁôΩÈªí„ÅØ„Å£„Åç„Çä„Å§„Åë„Çã„Éè„Éº„Éâ„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## üíé Vicara (`StandardVicara`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: Vitakka„ÅßÊé¢Á¥¢„Åï„Çå„ÅüÂàùÊúüÁä∂ÊÖã $\\mathbf{s}_0$ „ÇíËµ∑ÁÇπ„Å´„ÄÅÂÜÖÈÉ®„ÅßÊÄùËÄÉ„É´„Éº„Éó„ÇíÂõû„Åó„Å¶Áä∂ÊÖã„Çí\\*\\*Á¥îÂåñÔºàRefineÔºâ\\*\\*„Åó„ÄÅÊ≠£Â∏∏„Å™„Éë„Çø„Éº„É≥„Å´„Çà„ÇäËøë„ÅÑÁä∂ÊÖã $\\mathbf{s}_{\\text{final}}$ „ÇíÂ∞éÂá∫„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `standard`\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"vicara\": {\n",
    "    \"refine_steps\": 5,                 // Áä∂ÊÖãÁ¥îÂåñ„ÅÆÁπ∞„ÇäËøî„Åó„Çπ„ÉÜ„ÉÉ„ÉóÊï∞ÔºàÊÄùËÄÉÂõûÊï∞Ôºâ\n",
    "    \"refiner_type\": \"mlp\",             // ÂÜÖÈÉ®„ÅÆRefiner„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºàÂ§öÂ±§„Éë„Éº„Çª„Éó„Éà„É≠„É≥Ôºâ\n",
    "    \"inertia\": 0.7                     // Áä∂ÊÖãÊõ¥Êñ∞„ÅÆÊÖ£ÊÄßÔºàÊÄ•ÊøÄ„Å™Â§âÂåñ„ÇíÊäë„Åà„ÇãÔºâ\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## üß† Refiners (`MlpRefiner`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: Vicara„ÅÆÂÜÖÈÉ®„Åß‰ΩøÁî®„Åï„Çå„Çã„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åß„ÄÅÊΩúÂú®Áä∂ÊÖã„Çí**ÂÜçÂ∏∞ÁöÑ„Å´Ê¥óÁ∑¥**„Åï„Åõ„ÇãÂΩπÂâ≤„ÇíÊãÖ„ÅÑ„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `mlp` (Â§öÂ±§„Éë„Éº„Çª„Éó„Éà„É≠„É≥)\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**: SamadhiConfig„ÅÆ„Ç∞„É≠„Éº„Éê„É´„Å™ `dim` (**32**) „ÅåÂÜÖÈÉ®ÁöÑ„Å´‰ºùÊê¨„Åï„Çå„ÄÅRefiner„ÅÆÈö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉ„Å™„Å©„Å´ÂΩ±Èüø„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "-----\n",
    "\n",
    "## üéØ Objective (Â≠¶ÁøíÁõÆÊ®ô)\n",
    "\n",
    "„Åì„ÅÆ„Éá„É¢„Åß„ÅØ2„Å§„ÅÆ Objective „Åå‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ\n",
    "\n",
    "### Phase 1: ‰∫ãÂâçÂ≠¶ÁøíÁî® (`AutoencoderObjective`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: Adapter„Å®Decoder„ÅÆÊïôÂ∏´„Å™„Åó‰∫ãÂâçÂ≠¶ÁøíÊôÇ„Å´‰ΩøÁî®„Åï„Çå„ÇãÊêçÂ§±Èñ¢Êï∞„ÄÇ‰∏ª„Å´**ÂÜçÊßãÊàêË™§Â∑Æ„ÅÆ„Åø**„ÇíÊúÄÂ∞èÂåñ„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `AutoencoderObjective`\n",
    "  * **ÁâπÂæ¥**: Vitakka„Å®Vicara„ÅÆ„Éó„É≠„Çª„Çπ„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô (`needs_vitakka=False`, `needs_vicara=False`)„ÄÇ\n",
    "\n",
    "### Phase 3: „É°„Ç§„É≥Â≠¶ÁøíÁî® (`AnomalyObjective`)\n",
    "\n",
    "  * **ÂΩπÂâ≤**: „É¢„Éá„É´ÂÖ®‰Ωì„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÊôÇ„Å´‰ΩøÁî®„Åï„Çå„ÇãÊêçÂ§±Èñ¢Êï∞„ÄÇ**ÂÜçÊßãÊàêË™§Â∑Æ**„Å´Âä†„Åà„ÄÅÁä∂ÊÖã„ÅÆ**ÂÆâÂÆöÊÄß**„ÄÅ„Éó„É≠„Éº„ÉñÈÅ∏Êäû„ÅÆ**„Ç®„É≥„Éà„É≠„Éî„Éº**„ÄÅ„Éó„É≠„Éº„ÉñÂà©Áî®„ÅÆ**„É≠„Éº„Éâ„Éê„É©„É≥„Çπ**„ÇíËÄÉÊÖÆ„Åó„Åæ„Åô„ÄÇ\n",
    "  * **„Çø„Ç§„Éó**: `AnomalyObjective`\n",
    "  * **‰∏ª„Å™Ë®≠ÂÆö**:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"objective\": {\n",
    "    \"stability_coeff\": 0.1,    // ÊΩúÂú®Áä∂ÊÖã„ÅÆÂÆâÂÆöÊÄß„Å´ÂØæ„Åô„ÇãÈáç„ÅøÔºà„Éñ„É¨„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„ÇãÔºâ\n",
    "    \"entropy_coeff\": 0.05,     // „Éó„É≠„Éº„ÉñÈÅ∏Êäû„ÅÆÊõñÊòß„Åï„ÇíÊ∏õ„Çâ„Åô\n",
    "    \"balance_coeff\": 1.0,      // ÁâπÂÆö„ÅÆ„Éó„É≠„Éº„Éñ„Å∏„ÅÆÈÅéÂ∫¶„Å™ÈõÜ‰∏≠„ÇíÈò≤„Åê\n",
    "    \"anomaly_margin\": 10.0,    // Áï∞Â∏∏Ê§úÁü•Áî®„Éû„Éº„Ç∏„É≥ (Samadhi„É¢„Éá„É´„ÅÆObjectiveÂÜÖ„ÅßÂá¶ÁêÜ„Åï„Çå„ÇãÂ†¥Âêà)\n",
    "    \"anomaly_weight\": 1.0\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. „É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
    "\n",
    "ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Å®„ÄÅSamadhi„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆ„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÇíË™≠„ÅøËæº„Åø„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from src.presets.tabular import create_mlp_samadhi\n",
    "from src.configs.main import SamadhiConfig\n",
    "from src.train import SamadhiTrainer # AnomalySamadhiTrainer„ÅÆ‰ª£„Çè„Çä„Å´SamadhiTrainer„Çí‰ΩøÁî®\n",
    "from src.train.objectives.autoencoder import AutoencoderObjective # Phase 1Áî®„Å´ËøΩÂä†\n",
    "from src.train.objectives.anomaly import AnomalyObjective # Phase 3Áî®„Å´ËøΩÂä†\n",
    "\n",
    "# Hugging Face„ÅÆTrainingArguments„ÇÇÂøÖË¶Å\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# „Éá„Éê„Ç§„Çπ„ÅÆË®≠ÂÆö\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ë®≠ÂÆö (Configuration)\n",
    "\n",
    "‰∏çÊ≠£Ê§úÁü•„Çø„Çπ„ÇØÂêë„Åë„ÅÆ„É¢„Éá„É´Ë®≠ÂÆö„ÇíÂÆöÁæ©„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "* **Input Dim 29:** `creditcard.csv` „ÅÆÁâπÂæ¥ÈáèÊï∞ÔºàV1-V28 + AmountÔºâ„ÄÇ\n",
    "* **5 Probes:** Ê≠£Â∏∏„Å™ÂèñÂºï„Éë„Çø„Éº„É≥„Çí5„Å§„ÅÆ‰ª£Ë°®ÁöÑ„Å™„ÄåÊ¶ÇÂøµ„Äç„Å®„Åó„Å¶Ë°®Áèæ„Åó„Åæ„Åô„ÄÇ\n",
    "* **Anomaly Margin 10.0:** Áï∞Â∏∏Ê§úÁü•Â≠¶ÁøíÊôÇ„ÅÆ„Éû„Éº„Ç∏„É≥„ÄÇÊ≠£Â∏∏„Éá„Éº„Çø„Å®Áï∞Â∏∏„Éá„Éº„Çø„Çí„Å©„Çå„Åè„Çâ„ÅÑÂºï„ÅçÈõ¢„Åô„Åã„ÇíÂà∂Âæ°„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„Éë„Çπ (Áí∞Â¢É„Å´Âêà„Çè„Åõ„Å¶Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ)\n",
    "DATA_PATH = \"../data/creditcard.csv\"\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS_PRETRAIN = 10\n",
    "EPOCHS_MAIN = 10\n",
    "LATENT_DIM = 32\n",
    "N_PROBES = 5\n",
    "\n",
    "CONFIG_DICT = {\n",
    "    \"dim\": LATENT_DIM,\n",
    "\n",
    "    # Adapter (MLP): Ë°®ÂΩ¢Âºè„Éá„Éº„ÇøÁî® - ÂÖ•ÂäõÁâπÂæ¥Èáè„ÇíÊΩúÂú®Á©∫Èñì„Å∏Â§âÊèõ\n",
    "    \"adapter\": {\n",
    "        \"type\": \"mlp\",\n",
    "        \"input_dim\": 29,\n",
    "        \"hidden_dim\": 64\n",
    "    },\n",
    "\n",
    "    # Decoder (Reconstruction): ÊΩúÂú®Ë°®Áèæ„Åã„ÇâÂÖ•Âäõ„ÇíÂæ©ÂÖÉ\n",
    "    \"decoder\": {\n",
    "        \"type\": \"reconstruction\",\n",
    "        \"input_dim\": 29,\n",
    "        \"decoder_hidden_dim\": 64\n",
    "    },\n",
    "\n",
    "    # Vitakka: Ê≠£Â∏∏„Éë„Çø„Éº„É≥„ÅÆÊé¢Á¥¢ÔºàÊ¶ÇÂøµÊé¢Á¥¢Ôºâ\n",
    "    \"vitakka\": {\n",
    "        \"n_probes\": N_PROBES,\n",
    "        \"gate_threshold\": 0.5,\n",
    "        \"mix_alpha\": 0.5,\n",
    "        \"softmax_temp\": 0.5,\n",
    "        \"training_attention_mode\": \"soft\",\n",
    "        \"prediction_attention_mode\": \"hard\",\n",
    "    },\n",
    "\n",
    "    # Vicara: Áä∂ÊÖãÁ¥îÂåñÔºàÊ¶ÇÂøµ„Å∏„ÅÆÂºï„ÅçÂØÑ„ÅõÔºâ\n",
    "    \"vicara\": {\n",
    "        \"refine_steps\": 5,\n",
    "        \"refiner_type\": \"mlp\",\n",
    "        \"inertia\": 0.7\n",
    "    },\n",
    "\n",
    "    # Objective (Anomaly DetectionÁî®) - Áï∞Â∏∏Ê§úÁü•„ÅÆ„Åü„ÇÅ„ÅÆËøΩÂä†ÊêçÂ§±\n",
    "    \"objective\": {\n",
    "        \"stability_coeff\": 0.1,\n",
    "        \"entropy_coeff\": 0.05,\n",
    "        \"balance_coeff\": 1.0,\n",
    "        \"anomaly_margin\": 10.0, # Ê≠£Â∏∏„Å®Áï∞Â∏∏„ÅÆÂàÜÈõ¢„Éû„Éº„Ç∏„É≥\n",
    "        \"anomaly_weight\": 1.0,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. „Éá„Éº„ÇøÊ∫ñÂÇô (Data Loading & Preprocessing)\n",
    "\n",
    "„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„Åø„ÄÅÊ®ôÊ∫ñÂåñÔºàStandardScalerÔºâ„ÇíË°å„ÅÑ„ÄÅÂ≠¶ÁøíÁî®„Å®„ÉÜ„Çπ„ÉàÁî®„Å´ÂàÜÂâ≤„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "* **Â≠¶Áøí„Éá„Éº„Çø:** Ê≠£Â∏∏„Éá„Éº„Çø80% + ‰∏çÊ≠£„Éá„Éº„Çø50%ÔºàÂ∞ëÈáèÊ∑∑„Åú„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÈ†ëÂÅ•„Å™Â¢ÉÁïå„ÇíÂ≠¶Áøí„Åï„Åõ„Åæ„ÅôÔºâ\n",
    "* **„ÉÜ„Çπ„Éà„Éá„Éº„Çø:** ÊÆã„Çä„ÅÆÊ≠£Â∏∏„Éá„Éº„Çø20% + ÊÆã„Çä„ÅÆ‰∏çÊ≠£„Éá„Éº„Çø50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset # Dataset„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÅØÂøÖÈ†à\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÆöÁæ©\n",
    "# ---------------------------------------------------------\n",
    "class AnomalyDataset(Dataset):\n",
    "    \"\"\"SamadhiTrainer (Hugging Face Trainer base)Áî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇËæûÊõ∏ÂΩ¢Âºè„ÅßËøî„Åô„ÄÇ\"\"\"\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        # „É©„Éô„É´y„Åå„Å™„ÅÑÂ†¥ÂêàÔºà‰æãÔºöPhase 3„ÅÆOne-ClassÂ≠¶ÁøíÊôÇÔºâ„ÄÅTrainer„ÅåË¶ÅÊ±Ç„Åô„Çã„ÉÄ„Éü„Éº„ÅÆy„Çí‰ΩúÊàê\n",
    "        self.y = y if y is not None else x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Trainer„ÅåÁâπÂæ¥Èáè„Çí\"x\"„ÄÅ„É©„Éô„É´„Çí\"y\"„Å®„Åó„Å¶Ë™çË≠ò„Åß„Åç„Çã„Çà„ÅÜ„Å´ËæûÊõ∏ÂΩ¢Âºè„ÅßËøî„Åô\n",
    "        item = {\"x\": self.x[idx]}\n",
    "        if self.y is not None:\n",
    "            item[\"y\"] = self.y[idx]\n",
    "        return item\n",
    "\n",
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    # TensorDataset„Åß„ÅØ„Å™„ÅèAnomalyDataset„Çí‰Ωø„ÅÜ„Çà„ÅÜ„Å´Â§âÊõ¥\n",
    "    dataset = AnomalyDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def generate_dummy_data():\n",
    "    \"\"\"„Éá„Éº„Çø„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„Å´„ÉÄ„Éü„Éº„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁîüÊàê„Åô„Çã\"\"\"\n",
    "    n_samples = 10000\n",
    "    n_features = 29\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    y = torch.zeros(n_samples, dtype=torch.long)\n",
    "    # „Çè„Åö„Åã„Å´Áï∞Â∏∏„Éá„Éº„Çø„Çí‰ΩúÊàê„Åó„ÄÅÂàÜÂ∏É„Çí„Ç∑„Éï„Éà„Åï„Åõ„Çã\n",
    "    y[:50] = 1 # Fraud\n",
    "    X[:50] += 5.0\n",
    "\n",
    "    # Train (80%) „Å® Test (20%) „Å´ÂàÜÂâ≤\n",
    "    return X[:8000], y[:8000], X[8000:], y[8000:]\n",
    "\n",
    "def load_data_semi_supervised(path):\n",
    "    \"\"\"\n",
    "    „Éá„Éº„Çø„Éï„Ç°„Ç§„É´„Çí„É≠„Éº„Éâ„Åó„ÄÅÊ®ôÊ∫ñÂåñ„ÇíË°å„ÅÑ„ÄÅÂçäÊïôÂ∏´„ÅÇ„ÇäÂ≠¶ÁøíÁî®„ÅÆÊà¶Áï•„ÅßÂàÜÂâ≤„Åô„Çã„ÄÇ\n",
    "    - Train: Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ80% + ‰∏çÊ≠£„Éá„Éº„Çø„ÅÆ50%\n",
    "    - Test: Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ20% + ‰∏çÊ≠£„Éá„Éº„Çø„ÅÆ50%\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        # ÂÆüË°åÁí∞Â¢É„Å´„Çà„Å£„Å¶„Éë„Çπ„ÅåÁï∞„Å™„ÇãÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ\n",
    "        fallback_path = \"../\" + path\n",
    "        if os.path.exists(fallback_path):\n",
    "             df = pd.read_csv(fallback_path)\n",
    "        else:\n",
    "             print(\"Warning: Data file not found. Generating dummy data.\")\n",
    "             return generate_dummy_data()\n",
    "\n",
    "    # 'Time'„Ç´„É©„É†„ÇíÂâäÈô§\n",
    "    df = df.drop(columns=[\"Time\"])\n",
    "\n",
    "    # 'Amount'„ÇíÊ®ôÊ∫ñÂåñ\n",
    "    scaler = StandardScaler()\n",
    "    df[\"Amount\"] = scaler.fit_transform(df[\"Amount\"].values.reshape(-1, 1))\n",
    "\n",
    "    normal_df = df[df[\"Class\"] == 0]\n",
    "    fraud_df = df[df[\"Class\"] == 1]\n",
    "\n",
    "    # Split Normal (80/20)\n",
    "    train_normal, test_normal = train_test_split(normal_df, test_size=0.2, random_state=42)\n",
    "    # Split Fraud (50/50)\n",
    "    train_fraud, test_fraud = train_test_split(fraud_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Train Set (Normal + Fraud) - Â∞ëÈáè„ÅÆ‰∏çÊ≠£„ÇíÂ≠¶Áøí„Å´Ê∑∑„Åú„Çã (ÂçäÊïôÂ∏´„ÅÇ„Çä„ÅÆÊÑèÂõ≥)\n",
    "    train_df = pd.concat([train_normal, train_fraud])\n",
    "    # Test Set (Normal + Fraud)\n",
    "    test_df = pd.concat([test_normal, test_fraud])\n",
    "\n",
    "    print(f\"Train: Normal={len(train_normal)}, Fraud={len(train_fraud)}\")\n",
    "    print(f\"Test : Normal={len(test_normal)}, Fraud={len(test_fraud)}\")\n",
    "\n",
    "    # PyTorch„ÉÜ„É≥„ÇΩ„É´„Å´Â§âÊèõ\n",
    "    X_train = torch.tensor(train_df.drop(columns=[\"Class\"]).values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(train_df[\"Class\"].values, dtype=torch.long)\n",
    "\n",
    "    X_test = torch.tensor(test_df.drop(columns=[\"Class\"]).values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(test_df[\"Class\"].values, dtype=torch.long)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. „É¢„Éá„É´ÊßãÁØâ„Å®Ë©ï‰æ°Èñ¢Êï∞\n",
    "\n",
    "Samadhi„É¢„Éá„É´„ÇíÊßãÁØâ„Åó„ÄÅË©ï‰æ°„ÉªÂàÜÊûê„ÅÆ„Åü„ÇÅ„ÅÆÈñ¢Êï∞„ÇíÂÆöÁæ©„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. „É¢„Éá„É´ÊßãÁØâ\n",
    "config = SamadhiConfig.from_dict(CONFIG_DICT)\n",
    "model = create_mlp_samadhi(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# 2. Ë©ï‰æ°ÂÆüË°åÈñ¢Êï∞\n",
    "def evaluate_anomaly_detection_with_gate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Samadhi„É¢„Éá„É´„ÇíÂÆüË°å„Åó„ÄÅÂÜçÊßãÁØâË™§Â∑ÆÔºàÁï∞Â∏∏„Çπ„Ç≥„Ç¢Ôºâ„ÄÅ‰ø°È†ºÂ∫¶„ÄÅÂãùËÄÖProbe„ÄÅ„Ç≤„Éº„ÉàÈÄöÈÅé„Éï„É©„Ç∞„ÇíÂèéÈõÜ„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Samadhi„É¢„Éá„É´„Ç§„É≥„Çπ„Çø„É≥„ÇπÔºàAdapter, Vitakka, Vicara, Decoder„ÇíÊåÅ„Å§Ôºâ„ÄÇ\n",
    "        dataloader (torch.utils.data.DataLoader): „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÄÇÂêÑ„Éê„ÉÉ„ÉÅ„ÅØËæûÊõ∏ {\"x\": ...} ÂΩ¢Âºè„ÇíÊÉ≥ÂÆö„ÄÇ\n",
    "        device (str/torch.device): Êé®Ë´ñ„ÇíÂÆüË°å„Åô„Çã„Éá„Éê„Ç§„Çπ„ÄÇ\n",
    "\n",
    "    Returns:\n",
    "        tuple: (ÂÜçÊßãÁØâË™§Â∑Æ, ‰ø°È†ºÂ∫¶, ÂãùËÄÖ„Éó„É≠„Éº„ÉñID, „Ç≤„Éº„ÉàÈÄöÈÅé„Éï„É©„Ç∞) „ÅÆNumPyÈÖçÂàó\n",
    "    \"\"\"\n",
    "    # Ë©ï‰æ°„É¢„Éº„Éâ„Å´Ë®≠ÂÆö\n",
    "    model.eval()\n",
    "\n",
    "    reconstruction_errors = []\n",
    "    confidences = []\n",
    "    winner_probes = []\n",
    "    gate_opens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # ÂÖ•Âäõ„Éá„Éº„Çø x (Áîü„Éá„Éº„Çø) „Çí„Éá„Éê„Ç§„Çπ„Å∏Ëª¢ÈÄÅ\n",
    "            x = batch[\"x\"].to(device)\n",
    "\n",
    "            # 0. Adapter: Raw Input -> Latent Space (Áîü„ÅÆÂÖ•Âäõ„ÇíÊΩúÂú®„Éô„ÇØ„Éà„É´ z „Å´Â§âÊèõ)\n",
    "            z = model.adapter(x)\n",
    "\n",
    "            # A. Search (Vitakka): ÊΩúÂú®„Éô„ÇØ„Éà„É´ z „ÇíÂÖ•Âäõ„Å®„Åó„Å¶„ÄÅÊúÄ„ÇÇËøë„ÅÑ„ÄåÊ≠£Â∏∏Ê¶ÇÂøµÔºàProbeÔºâ„Äç„ÇíÊé¢Á¥¢\n",
    "            s0, meta = model.vitakka(z) # s0 „ÅØÂàùÊúüÁä∂ÊÖã„ÄÅmeta „ÅØÊé¢Á¥¢ÁµêÊûú„ÅÆ„É°„Çø„Éá„Éº„Çø\n",
    "            conf = meta[\"confidence\"] # Ê≠£Â∏∏Ê¶ÇÂøµ„Å∏„ÅÆÁ¢∫‰ø°Â∫¶\n",
    "            winner = meta[\"winner_id\"] # ÊúÄ„ÇÇËøë„ÅÑ Probe „ÅÆ ID\n",
    "            is_open = meta[\"gate_open\"] # Ê¶ÇÂøµ„Å®„Åó„Å¶ÂçÅÂàÜ„Å´Ëøë„ÅÑ„Åã„Å©„ÅÜ„Åã„ÅÆÂà§ÂÆö (True/False)\n",
    "\n",
    "            # B. Refine (Vicara): ÂàùÊúüÁä∂ÊÖã s0 „Çí„ÄÅVitakka„ÅßÂæó„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà („Å©„ÅÆÊ¶ÇÂøµ„Å´Â±û„Åô„Çã„Åã) „Å´Âü∫„Å•„ÅÑ„Å¶Á¥îÂåñ\n",
    "            s_final, _, _ = model.vicara(s0, context=meta)\n",
    "\n",
    "            # C. Reconstruct (Decoder): Á¥îÂåñ„Åï„Çå„ÅüÊúÄÁµÇÁä∂ÊÖã s_final „Åã„ÇâÂÖÉ„ÅÆÂÖ•ÂäõÁ©∫Èñì„Å∏Âæ©ÂÖÉ\n",
    "            x_recon = model.decoder(s_final)\n",
    "\n",
    "            # Metrics: ÂÜçÊßãÁØâË™§Â∑Æ (MSE) „ÇíË®àÁÆó„Åó„ÄÅÁï∞Â∏∏„Çπ„Ç≥„Ç¢„Å®„Åó„Å¶‰ΩøÁî®\n",
    "            # MSE = (x - x_recon)^2 „ÅÆÂπ≥Âùá\n",
    "            mse = torch.mean((x - x_recon) ** 2, dim=1)\n",
    "\n",
    "            # ÁµêÊûú„Çí„É™„Çπ„Éà„Å´Ê†ºÁ¥ç\n",
    "            reconstruction_errors.append(mse.cpu())\n",
    "            confidences.append(conf.cpu())\n",
    "            winner_probes.append(winner.cpu())\n",
    "            gate_opens.append(is_open.cpu())\n",
    "\n",
    "    # ÂÖ®„Éê„ÉÉ„ÉÅ„ÅÆÁµêÊûú„ÇíÁµêÂêà„Åó„ÄÅNumPyÈÖçÂàó„Å®„Åó„Å¶Ëøî„Åô\n",
    "    return (\n",
    "        torch.cat(reconstruction_errors).numpy(),\n",
    "        torch.cat(confidences).numpy(),\n",
    "        torch.cat(winner_probes).numpy(),\n",
    "        torch.cat(gate_opens).numpy()\n",
    "    )\n",
    "\n",
    "# Note: „Åì„ÅÆÈñ¢Êï∞„Çí‰ΩøÁî®„Åô„ÇãÈöõ„ÅØ„ÄÅÂ§ñÈÉ®„Åß model, dataloader, device „ÇíÂÆöÁæ©„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "# 3. ÂàÜÊûê„Éª„Éó„É≠„ÉÉ„ÉàÈñ¢Êï∞\n",
    "def analyze_and_plot_results(y_true, recon_errs, confs, winners, gates):\n",
    "    # --- Gating Analysis ---\n",
    "    print(\"\\n--- Gating Analysis (Passed Gate) ---\")\n",
    "    normal_mask = (y_true == 0)\n",
    "    fraud_mask = (y_true == 1)\n",
    "\n",
    "    print(f\"Normal: {np.sum(gates[normal_mask])}/{np.sum(normal_mask)} passed ({np.mean(gates[normal_mask]):.2%})\")\n",
    "    print(f\"Fraud : {np.sum(gates[fraud_mask])}/{np.sum(fraud_mask)} passed ({np.mean(gates[fraud_mask]):.2%})\")\n",
    "\n",
    "    # --- Reconstruction Error Stats ---\n",
    "    print(\"\\n--- Mean Reconstruction Error ---\")\n",
    "    print(f\"Normal: {np.mean(recon_errs[normal_mask]):.4f}\")\n",
    "    print(f\"Fraud : {np.mean(recon_errs[fraud_mask]):.4f}\")\n",
    "\n",
    "    # --- ROC-AUC ---\n",
    "    roc_recon = roc_auc_score(y_true, recon_errs)\n",
    "    print(f\"\\nROC-AUC Score: {roc_recon:.4f}\")\n",
    "\n",
    "    # --- Optimal Threshold (F1-Max) ---\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, recon_errs)\n",
    "    f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    print(f\"Best Threshold (F1-Max): {best_threshold:.4f}\")\n",
    "\n",
    "    # --- Visualization ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # „Éó„É≠„ÉÉ„ÉàÁØÑÂõ≤„Çí0„Åã„Çâ10„Å´„ÇØ„É™„ÉÉ„Éó„Åó„Å¶Ë¶ã„ÇÑ„Åô„Åè„Åô„Çã\n",
    "    sns.kdeplot(recon_errs[normal_mask], fill=True, label=\"Normal\", clip=(0, 10))\n",
    "    sns.kdeplot(recon_errs[fraud_mask], fill=True, label=\"Fraud\", clip=(0, 10))\n",
    "    plt.title(\"Reconstruction Error Distribution\")\n",
    "    plt.xlabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # „Çµ„É≥„Éó„É´Êï∞„ÅåÂ§ö„Åô„Åé„Çã„Åü„ÇÅ„ÄÅNormal„ÅØ‰∏ÄÈÉ®„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„Å¶„Éó„É≠„ÉÉ„Éà\n",
    "    plt.scatter(confs[normal_mask][:1000], recon_errs[normal_mask][:1000], alpha=0.3, label=\"Normal\", s=10)\n",
    "    plt.scatter(confs[fraud_mask], recon_errs[fraud_mask], alpha=0.6, label=\"Fraud\", s=20, color=\"red\")\n",
    "    plt.title(\"Confidence vs Recon Error\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Reconstruction Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Â≠¶ÁøíÂÆüË°å\n",
    "\n",
    "### Phase 1: ÊïôÂ∏´„Å™„Åó‰∫ãÂâçÂ≠¶Áøí (Autoencoder Pre-training)\n",
    "Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø„Çí‰Ωø„Å£„Å¶„ÄÅÂÖ•ÂäõÁâπÂæ¥Èáè„ÅÆÂúßÁ∏Æ„ÉªÂæ©ÂÖÉÔºàAdapter-DecoderÔºâ„ÇíÂ≠¶Áøí„Åï„Åõ„Åæ„Åô„ÄÇ\n",
    "\n",
    "### Phase 2: „Éó„É≠„Éº„ÉñÂàùÊúüÂåñ\n",
    "Â≠¶ÁøíÊ∏à„ÅøAdapter„ÅßÊ≠£Â∏∏„Éá„Éº„Çø„ÇíÊΩúÂú®Á©∫Èñì„Å´ÂÜôÂÉè„Åó„ÄÅ„ÇØ„É©„Çπ„Çø„É™„É≥„Ç∞ÔºàK-MeansÔºâ„ÇíË°å„Å£„Å¶„ÄåÊ≠£Â∏∏Ê¶ÇÂøµÔºàProbeÔºâ„Äç„ÅÆÂàùÊúüÂÄ§„ÇíÊ±∫ÂÆö„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "### Phase 3: „É°„Ç§„É≥Â≠¶Áøí (Anomaly Training)\n",
    "Áï∞Â∏∏Ê§úÁü•Áî®„ÅÆÊêçÂ§±Èñ¢Êï∞„ÇíÁî®„ÅÑ„Å¶„ÄÅ„É¢„Éá„É´ÂÖ®‰Ωì„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„É≠„Éº„Éâ\n",
    "X_train, y_train, X_test, y_test = load_data_semi_supervised(DATA_PATH)\n",
    "train_loader = create_dataloader(X_train, y_train, batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = create_dataloader(X_test, y_test, batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Phase 1: Autoencoder Pre-training (Normal Data Only)\n",
    "# ---------------------------------------------------------\n",
    "# ---------------------------------------------------------\n",
    "# Phase 1: Autoencoder Pre-training (Normal Data Only)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=== Phase 1: Autoencoder Pre-training ===\")\n",
    "print(\"Training Adapter & Decoder with Normal data only...\")\n",
    "\n",
    "# Ê≠£Â∏∏„Éá„Éº„Çø„ÅÆ„Åø„ÇíÊäΩÂá∫\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "y_train_normal = y_train[y_train == 0]\n",
    "\n",
    "train_ae_dataset = AnomalyDataset(X_train_normal, y_train_normal)\n",
    "\n",
    "# Objective & Trainer Setup\n",
    "# AutoencoderObjective„ÅØÂÜçÊßãÊàêÊêçÂ§±„ÅÆ„Åø„Çí‰ΩøÁî®\n",
    "ae_objective = AutoencoderObjective(config, device=device)\n",
    "\n",
    "pretrain_args = TrainingArguments(\n",
    "    num_train_epochs=EPOCHS_PRETRAIN,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_steps=3000,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"y\"],\n",
    "    use_mps_device=(device == \"mps\"),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "pretrainer = SamadhiTrainer(\n",
    "    model=model,\n",
    "    args=pretrain_args,\n",
    "    objective=ae_objective,\n",
    "    train_dataset=train_ae_dataset,\n",
    "    optimizers=(optim.Adam(model.parameters(), lr=1e-3), None)\n",
    ")\n",
    "\n",
    "pretrainer.train() # ÂÆüÈöõ„Å´„ÅØ„Åì„Åì„ÅßÂ≠¶Áøí„ÇíÂÆüË°å\n",
    "print(\"Pre-training completed. (Skipping actual training for demonstration)\") # „ÉÄ„Éü„ÉºÂÆüË°å„ÅÆÂ†¥Âêà\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Phase 2: Probe Initialization\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=== Phase 2: Probe Initialization ===\")\n",
    "print(\"Clustering normal data in latent space to find concept probes...\")\n",
    "\n",
    "model.eval()\n",
    "latents = []\n",
    "# Pre-training„Å´‰Ωø„Å£„ÅüÊ≠£Â∏∏„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„Çí‰ΩúÊàê„Åó„Å¶‰ΩøÁî®\n",
    "train_ae_loader = DataLoader(train_ae_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_ae_loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        # Adapter („Ç®„É≥„Ç≥„Éº„ÉÄÈÉ®ÂàÜ)„Çí‰ΩøÁî®„Åó„Å¶ÊΩúÂú®Á©∫Èñì„Å∏ÂÜôÂÉè\n",
    "        z = model.adapter(x)\n",
    "        latents.append(z.cpu().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0)\n",
    "\n",
    "# „Éá„Éº„Çø„ÅåÂ§ö„Åô„Åé„ÇãÂ†¥Âêà„ÅØ„Çµ„É≥„Éó„É™„É≥„Ç∞\n",
    "if len(latents) > 10000:\n",
    "    indices = np.random.choice(len(latents), 10000, replace=False)\n",
    "    latents_sub = latents[indices]\n",
    "else:\n",
    "    latents_sub = latents\n",
    "\n",
    "# K-Means„Åß„Éó„É≠„Éº„Éñ‰∏≠ÂøÉ„ÇíÊ±∫ÂÆö\n",
    "kmeans = KMeans(n_clusters=CONFIG_DICT[\"vitakka\"][\"n_probes\"], random_state=42, n_init=10)\n",
    "kmeans.fit(latents_sub)\n",
    "centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n",
    "\n",
    "# „É¢„Éá„É´„Å´„Éó„É≠„Éº„ÉñÔºàÊ≠£Â∏∏Ê¶ÇÂøµ„ÅÆ‰ª£Ë°®ÁÇπÔºâ„Çí„É≠„Éº„Éâ\n",
    "model.vitakka.load_probes(centroids)\n",
    "print(\"Probes initialized.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Phase 3: Main Training (Anomaly Detection)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=== Phase 3: Main Training (Anomaly Detection) ===\")\n",
    "print(\"Training full model to refine normal patterns...\")\n",
    "\n",
    "# „É°„Ç§„É≥Â≠¶ÁøíÁî®„ÅÆObjective (Vitakka + Vicara + Reconstruction + Áï∞Â∏∏„Éû„Éº„Ç∏„É≥)\n",
    "anomaly_objective = AnomalyObjective(config, device=device)\n",
    "\n",
    "# „É°„Ç§„É≥Â≠¶ÁøíÁî®„ÅÆDataset\n",
    "train_main_dataset = AnomalyDataset(X_train, y_train)\n",
    "\n",
    "main_args = TrainingArguments(\n",
    "    num_train_epochs=EPOCHS_MAIN,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_steps=3000,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"y\"],\n",
    "    use_mps_device=(device == \"mps\"),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "main_trainer = SamadhiTrainer(\n",
    "    model=model,\n",
    "    args=main_args,\n",
    "    objective=anomaly_objective,\n",
    "    train_dataset=train_main_dataset,\n",
    "    optimizers=(optim.Adam(model.parameters(), lr=1e-3), None)\n",
    ")\n",
    "\n",
    "main_trainer.train() # ÂÆüÈöõ„Å´„ÅØ„Åì„Åì„ÅßÂ≠¶Áøí„ÇíÂÆüË°å\n",
    "print(\"Main training completed. (Skipping actual training for demonstration)\") # „ÉÄ„Éü„ÉºÂÆüË°å„ÅÆÂ†¥Âêà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ÁµêÊûúÂàÜÊûê„Å®„ÉÜ„Çπ„ÉàË©ï‰æ°\n",
    "\n",
    "Â≠¶Áøí„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶ÊúÄÈÅ©„Å™ÈñæÂÄ§„ÇíÊ±∫ÂÆö„Åó„ÄÅ„Åù„ÅÆÈñæÂÄ§„Çí‰Ωø„Å£„Å¶„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆ‰∏çÊ≠£Ê§úÁü•ÊÄßËÉΩ„ÇíË©ï‰æ°„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Â≠¶Áøí„Éá„Éº„Çø„ÅßÈñæÂÄ§„ÇíÊ±∫ÂÆö\n",
    "print(\"\\n--- Analyzing Training Data to determine threshold ---\")\n",
    "recon_train, conf_train, win_train, gate_train = evaluate_anomaly_detection_with_gate(model, train_loader, device=device)\n",
    "# Ë©ï‰æ°ÁµêÊûú„ÅÆ„Éó„É≠„ÉÉ„Éà„Å®ÊúÄÈÅ©ÈñæÂÄ§„ÅÆË®àÁÆó\n",
    "best_threshold = analyze_and_plot_results(y_train.numpy(), recon_train, conf_train, win_train, gate_train)\n",
    "\n",
    "# 2. „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Åß„ÅÆÊúÄÁµÇË©ï‰æ°\n",
    "print(\"\\n--- Final Evaluation on Test Data ---\")\n",
    "recon_test, _, _, _ = evaluate_anomaly_detection_with_gate(model, test_loader, device=device)\n",
    "y_true_test = y_test.numpy()\n",
    "\n",
    "# ‰∫àÊ∏¨ (ÈñæÂÄ§Âà§ÂÆö)\n",
    "y_pred_test = (recon_test > best_threshold).astype(int)\n",
    "\n",
    "# „É¨„Éù„Éº„ÉàË°®Á§∫\n",
    "print(f\"Using Threshold: {best_threshold:.4f}\")\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=[\"Normal\", \"Fraud\"]))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_test, y_pred_test))\n",
    "\n",
    "roc_test = roc_auc_score(y_true_test, recon_test)\n",
    "print(f\"\\nTest ROC-AUC: {roc_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samadhi-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
